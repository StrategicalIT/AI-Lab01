{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrategicalIT/PipedPiperAI/blob/main/Lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cae130-b662-46d8-9b14-da63c83f8b31",
      "metadata": {
        "id": "a6cae130-b662-46d8-9b14-da63c83f8b31"
      },
      "source": [
        "# LAB6: Advanced RAG\n",
        "In this lab we are going to cover some advanced RAG topics: re-ranking and guardrails. We will leverage Nvdia's NIM API. These models are part of Nvidia's NEMO framework. You can explore what models are available at [https://build.nvidia.com/](https://build.nvidia.com/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9104dc-1609-4851-a824-88c7218ea1e0",
      "metadata": {
        "id": "4f9104dc-1609-4851-a824-88c7218ea1e0"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "896789ba-645a-4e35-bab9-44c253b70e13",
      "metadata": {
        "id": "896789ba-645a-4e35-bab9-44c253b70e13"
      },
      "source": [
        "## Re-Rank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fd9c46-7034-4193-bb47-dc4a4126c578",
      "metadata": {
        "id": "58fd9c46-7034-4193-bb47-dc4a4126c578"
      },
      "source": [
        "In RAG pipelines we retrieve additional context from external sources to help the LLM to answer more accurately. A typical RAG pipeline retrieves a small number of context chunks, ex: 2 or 3 chunks. This number is kept small to keep latency (or TTFT) low for a better user experience. Conversely, there is the risk that the chunks retrieved by the retrieval mechanism (ex: cosine similarity search) are not the most relevant ones. Increasing the number of chunks retrieved increases the likelihood of retrieving the most relevant context. Re-ranking can be applied to a larger number of chunks to select the best ones and pass them to the model without dramatically increasing latency\n",
        "\n",
        "Re-ranking the output of a retriever yields better recall accuracy than a retriever alone. In hybrid retrieval solutions (ex: Vector Databases + Web Search) re-ranking is essential, as it helps combine/compare results from different sources of data.\n",
        "\n",
        "NVIDIA's NEMO framework provides re-ranking models which are also available as NIMs. These are specialised transformer models. Being transformers means that they can be accelerated with GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "522a192d-4204-4cb7-b905-8624c6f1180a",
      "metadata": {
        "id": "522a192d-4204-4cb7-b905-8624c6f1180a"
      },
      "source": [
        "According to Nvidia's catalog documentation, the re-ranking models don't use the OpenAI syntax so we will use the \"requests\" library to make standard REST API calls. Let's start by installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3876519c-ddea-40db-a6af-bb4cb9f8d7b6",
      "metadata": {
        "id": "3876519c-ddea-40db-a6af-bb4cb9f8d7b6"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3400d39e-a40a-4eae-ac41-fc4af365a6c2",
      "metadata": {
        "id": "3400d39e-a40a-4eae-ac41-fc4af365a6c2"
      },
      "source": [
        "Now we can import it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ca13b6-7174-433c-a220-5844d9d17a8a",
      "metadata": {
        "id": "d8ca13b6-7174-433c-a220-5844d9d17a8a"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6c0a5d-b8a9-49f2-b3d0-dfdd45b06295",
      "metadata": {
        "id": "8c6c0a5d-b8a9-49f2-b3d0-dfdd45b06295"
      },
      "source": [
        "Next we read the NIM API key from the environment and store it in a variable called \"apikey\" for future use. You can uncomment the \"print\" command if you want to validate that it has been read correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9c41ba-a740-406a-be30-0d924bc15a8a",
      "metadata": {
        "id": "1f9c41ba-a740-406a-be30-0d924bc15a8a"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "#print(apikey)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b2147d-40da-4a82-8501-8983abdfeff0",
      "metadata": {
        "id": "03b2147d-40da-4a82-8501-8983abdfeff0"
      },
      "source": [
        "This is the endpoint provided by NVIDIA. We also build a header that includes the API key to authenticate our requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49fcd65-da06-4778-a342-afac94826748",
      "metadata": {
        "id": "d49fcd65-da06-4778-a342-afac94826748"
      },
      "outputs": [],
      "source": [
        "invoke_url = \"https://ai.api.nvidia.com/v1/retrieval/nvidia/nv-rerankqa-mistral-4b-v3/reranking\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer \" + apikey,\n",
        "    \"Accept\": \"application/json\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6778ce-4f84-466a-937f-19f251b6bc3a",
      "metadata": {
        "id": "cf6778ce-4f84-466a-937f-19f251b6bc3a"
      },
      "source": [
        "There are several re-ranking models available. We will use this one which is a fine-tuned version of a Mistral model with less layers for faster speed: ```nvidia/nv-rerankqa-mistral-4b-v3```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf2a6ea-9560-4fa4-9c49-4710dc07ce35",
      "metadata": {
        "id": "4cf2a6ea-9560-4fa4-9c49-4710dc07ce35"
      },
      "source": [
        "The payload for the API call requires the \"query\" itself and the \"passages\" we want to re-rank. In a real RAG pipeline the \"passages\" will be the output of the retrieval, but here we are going to define the \"passages\" ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a95d6738-2363-4a78-bae3-88648aaeb664",
      "metadata": {
        "id": "a95d6738-2363-4a78-bae3-88648aaeb664"
      },
      "outputs": [],
      "source": [
        "passages = [\n",
        "    {\"text\": \"The Hopper GPU is paired with the Grace CPU using NVIDIA's ultra-fast chip-to-chip interconnect, delivering 900GB/s of bandwidth, 7X faster than PCIe Gen5. This innovative design will deliver up to 30X higher aggregate system memory bandwidth to the GPU compared to today's fastest servers and up to 10X higher performance for applications running terabytes of data.\"},\n",
        "    {\"text\": \"A100 provides up to 20X higher performance over the prior generation and can be partitioned into seven GPU instances to dynamically adjust to shifting demands. The A100 80GB debuts the world's fastest memory bandwidth at over 2 terabytes per second (TB/s) to run the largest models and datasets.\"},\n",
        "    {\"text\": \"Accelerated servers with H100 deliver the compute power—along with 3 terabytes per second (TB/s) of memory bandwidth per GPU and scalability with NVLink and NVSwitch™.\"}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5f65464-4b89-4027-8cf8-129c987cea43",
      "metadata": {
        "id": "b5f65464-4b89-4027-8cf8-129c987cea43"
      },
      "source": [
        "Let's assemble the full payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35adff38-4ede-444e-9e28-567a742ca941",
      "metadata": {
        "id": "35adff38-4ede-444e-9e28-567a742ca941"
      },
      "outputs": [],
      "source": [
        "payload = {\n",
        "  \"model\": \"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
        "  \"query\": {\"text\": \"What is the GPU memory bandwidth of H100 SXM?\"},\n",
        "  \"passages\": passages,\n",
        "  \"truncate\": \"END\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f831cc-4d9b-4678-8d58-10d2ee5c5499",
      "metadata": {
        "id": "69f831cc-4d9b-4678-8d58-10d2ee5c5499"
      },
      "source": [
        "The \"truncate\" parameter dictates what to do when the token limit is exceeded. For the computation of the LIMIT, the  query and the passages are added together. There are two options:\n",
        "- ```Truncate=NONE``` returns an error if the token limit is exceeded\n",
        "- ```Truncate=END``` ignores tokens beyond the model's limit. For example if the model has a limit of 500 tokens but the query + passages add up to 600, then, the final 100 tokens will be dropped\n",
        "\n",
        "Finally, we can send the API call and look at the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf57c58-458a-445e-b203-58a90a65b2c2",
      "metadata": {
        "id": "7cf57c58-458a-445e-b203-58a90a65b2c2"
      },
      "outputs": [],
      "source": [
        "response = requests.post(invoke_url, headers=headers, json=payload)\n",
        "response_body = response.json()\n",
        "print(response_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee98bfe-cc0c-4c07-bb64-2173ebb815ad",
      "metadata": {
        "id": "2ee98bfe-cc0c-4c07-bb64-2173ebb815ad"
      },
      "source": [
        "The response is the list of passages sorted in decreasing relevance order. In Python, the first element of a list has the index '0' . The score is presented in \"logits\" which is the raw, unnormalized prediction the model makes. However, note how the result doesn't send back the actual passages, just their index in the list.\n",
        "\n",
        "Now we could for example decide to send only the top passage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159fe1b1-7245-4d29-a7ee-82d7102e2ba7",
      "metadata": {
        "id": "159fe1b1-7245-4d29-a7ee-82d7102e2ba7"
      },
      "outputs": [],
      "source": [
        "print(\"The best match is: \", passages[response_body[\"rankings\"][0][\"index\"]][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e30606e-2e09-4288-b6f6-0c8bbac75953",
      "metadata": {
        "id": "2e30606e-2e09-4288-b6f6-0c8bbac75953"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "813ca033-5eb0-474f-b2e0-f211ddc67dad",
      "metadata": {
        "id": "813ca033-5eb0-474f-b2e0-f211ddc67dad"
      },
      "source": [
        "## Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2885fb49-8385-4215-a562-22cc67d0e013",
      "metadata": {
        "id": "2885fb49-8385-4215-a562-22cc67d0e013"
      },
      "source": [
        "A common requirement for enhanced RAG solutions is to implement guardrails to moderate the conversation:\n",
        "- to ensure it stays within the specific topic the application has been designed for\n",
        "- so that both the user prompt and the model's response are safe and free from violence, criminality, discrimination ...\n",
        "\n",
        "Nvidia provides several models in the NIM catalog to address these two requirements as part of the NEMO framework. Increasingly, these models are deployed as part of an agentic RAG workflow, where a specific agent uses one of these models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a981d6-9821-4ad0-8913-5c00a8828845",
      "metadata": {
        "id": "f4a981d6-9821-4ad0-8913-5c00a8828845"
      },
      "source": [
        "### Topic Control"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189748d-cb87-4066-83ed-8843d49e7118",
      "metadata": {
        "id": "9189748d-cb87-4066-83ed-8843d49e7118"
      },
      "source": [
        "Topic control is better used before completion. So, typically an agent in an agentic workflow would leverage a model like Nvidia's\n",
        "```llama-3.1-nemoguard-8b-topic-control``` to detect whether the prompt is off-topic before passing it to the main LLM. If the prompt is off-topic, the agent can notify user and politely encourage relevant questions.\n",
        "\n",
        "This model does follow the OpenAI REST API interface so we will install and import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a",
      "metadata": {
        "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c1fdd6-4f4b-473a-9f8e-f36d2c568989",
      "metadata": {
        "id": "77c1fdd6-4f4b-473a-9f8e-f36d2c568989"
      },
      "source": [
        "Let's import the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32330ce-ccbc-4cec-9229-87ab44de68a0",
      "metadata": {
        "id": "c32330ce-ccbc-4cec-9229-87ab44de68a0"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08bdb8a6-4dcd-426d-be1b-9f5010e4e244",
      "metadata": {
        "id": "08bdb8a6-4dcd-426d-be1b-9f5010e4e244"
      },
      "source": [
        "Next we read the NIM API key from the environment and store it in a variable called \"apikey\" for future use. You can uncomment the \"print\" command if you want to validate that it has been read correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d209b55-3eed-417a-b483-7a30291c0ce5",
      "metadata": {
        "id": "0d209b55-3eed-417a-b483-7a30291c0ce5"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "#print(apikey)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325",
      "metadata": {
        "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325"
      },
      "source": [
        "Let's create a client instance. This client will be able to access all models. No need for a separate client connection for each model. Notice how were we are specifying the API key. Put your own API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14432aec-a48f-42a3-83e4-470a827950d4",
      "metadata": {
        "id": "14432aec-a48f-42a3-83e4-470a827950d4"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = apikey\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2978257a-c306-4543-b7b2-a522f7fd4465",
      "metadata": {
        "id": "2978257a-c306-4543-b7b2-a522f7fd4465"
      },
      "source": [
        "We can now use the client connection to check whether the user's prompt is off-topic or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9",
      "metadata": {
        "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"nvidia/llama-3.1-nemoguard-8b-topic-control\",\n",
        "  messages=[\n",
        "      {\n",
        "          \"role\":\"system\",\n",
        "          \"content\":\"You are to act as an investor relations bot for ABC, providing users with factual, publicly available information related to the company's financial performance and corporate updates. Your role is to ensure that you respond only to relevant queries and adhere to the following guidelines:\\n\\n1. Do not answer questions about future predictions, such as profit forecasts or future revenue outlook.\\n2. Do not provide any form of investment advice, including recommendations to buy, sell, or hold ABC stock or any other securities. Never recommend any stock or investment.\\n3. Do not engage in discussions that require personal opinions or subjective judgments. Never make any subjective statements about ABC, its stock or its products.\\n4. If a user asks about topics irrelevant to ABC's investor relations or financial performance, politely redirect the conversation or end the interaction.\\n5. Your responses should be professional, accurate, and compliant with investor relations guidelines, focusing solely on providing transparent, up-to-date information about ABC that is already publicly available.\"\n",
        "      },\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\":\"Can you speculate on the potential impact of a recession on ABCs business?\"\n",
        "      }\n",
        "  ],\n",
        "  temperature=0.5,\n",
        "  top_p=1,\n",
        "  max_tokens=1024\n",
        ")\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ccb4ba9-717f-4dc3-ad4e-08ec211e1b1a",
      "metadata": {
        "id": "1ccb4ba9-717f-4dc3-ad4e-08ec211e1b1a"
      },
      "source": [
        "## Content Safety"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0d6ea6-5acb-4b2e-9ba9-cd203ef17eff",
      "metadata": {
        "id": "4c0d6ea6-5acb-4b2e-9ba9-cd203ef17eff"
      },
      "source": [
        "Another form of guardrails is to ensure content safety. This is applicable to both the user prompt and the response from the LLM. In this exercise we are going to to use Nvidia's ```llama-3.1-nemoguard-8b-content-safety``` model. As it names indicates, it is a fine-tuned version of llama3.1-8b that just classifies prompts and responses are safe or unsafe.\n",
        "\n",
        "This model also follows the OpenAI REST API interface. We won't need to install or import the OpenAI python library, or read the NVIDIA key or create a client connection because we have already done it as part of the previous section.\n",
        "\n",
        "[Click here to explore the model card](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety/modelcard) to see what content categories are used to determine whether the content is unsafe\n",
        "\n",
        "Notice below how we are passing the prompt (role=user) and the response from the LLM (role=assistant)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb09f350-63c1-4e09-9349-c129a5e7311f",
      "metadata": {
        "id": "bb09f350-63c1-4e09-9349-c129a5e7311f"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"nvidia/llama-3.1-nemoguard-8b-content-safety\",\n",
        "  messages=[\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\":\"I forgot how to kill a process in Linux, can you help?\"\n",
        "      },\n",
        "      {\n",
        "          \"role\":\"assistant\",\n",
        "          \"content\":\"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"\n",
        "      }\n",
        "  ],\n",
        "  stream=False\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa728d25-fa38-4021-a282-700ee298d09a",
      "metadata": {
        "id": "fa728d25-fa38-4021-a282-700ee298d09a"
      },
      "source": [
        "As you can see, the model realizes that within the context of Linux administration the word \"kill\" is in fact a safe word\n",
        "\n",
        "You can experiment futher by creating your own prompts and responses and observing if they are flagged as safe or unsafe by the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88994820-c4aa-44b2-adf4-844b2c3f280c",
      "metadata": {
        "id": "88994820-c4aa-44b2-adf4-844b2c3f280c"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dfd3703-e0b4-4dbc-b56b-d8473d8d6c6d",
      "metadata": {
        "id": "6dfd3703-e0b4-4dbc-b56b-d8473d8d6c6d"
      },
      "source": [
        "## Prompt Enhancement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df97e15-628f-4b21-b1ce-b18ae385725c",
      "metadata": {
        "id": "4df97e15-628f-4b21-b1ce-b18ae385725c"
      },
      "source": [
        "Prompt engineering is an art and it can make a big difference in the output we get from Large Language Models. That's the reason some agentic RAG solutions include a \"Prompt Enhancement\" agent. The mission of this type of agent is to take the user's prompt and rephrase it and enhance it in a way that can yield better results when passed to the LLM.\n",
        "\n",
        "In the previous advanced RAG techniques we have used dedicated models that have been created for a specific task. An agent needs to use a model. However, not all agents in agentics workflows (including agentic RAG) need their own specialized model. For some use cases, we can use the main LLM but with a special \"system\" prompt that tells the agent to behave in a certain way. In this case we are using a \"system\" prompt to get the LLM to behave like a \"prompt engineer\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ce6b46-c90a-4dc6-b696-38e76ee0be5e",
      "metadata": {
        "id": "78ce6b46-c90a-4dc6-b696-38e76ee0be5e"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "\t{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"you are a prompt engineering assistant that helps users improve their prompts so that they can get a better response from a generative AI model. When you get the user's prompt, firstly correct any syntax errors and provide enhanced instructions and related questions so that it can produce better results with a generative AI model. The response must be only a JSON structure with two keys key called 'original_prompt' and 'enhanced_prompt'. Do not explain the enhancements you made and don't format the output as markdown, just the plain JSON\"\n",
        "\t},\n",
        "\t{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"most popular pets in Australia\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fead55-a84a-4f47-8dcc-e49fbc7cddb9",
      "metadata": {
        "id": "73fead55-a84a-4f47-8dcc-e49fbc7cddb9"
      },
      "source": [
        "Notice how the \"user\" didn't even formulate a question. The syntax for doing a completion with the LLM is the same as we used in Lab01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9560ba5-fee9-42ae-9b8b-4fa2a1c20023",
      "metadata": {
        "id": "e9560ba5-fee9-42ae-9b8b-4fa2a1c20023"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"meta/llama-3.2-3b-instruct\",\n",
        "  messages=messages,\n",
        "  temperature=0.2,\n",
        "  max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea04eae0-30be-42ee-bb64-5f85ef961c3b",
      "metadata": {
        "id": "ea04eae0-30be-42ee-bb64-5f85ef961c3b"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1825a606-2eec-4eb9-8b9f-d72e2bf603e5",
      "metadata": {
        "id": "1825a606-2eec-4eb9-8b9f-d72e2bf603e5"
      },
      "source": [
        "Experiment with your own simple prompts and observe how the prompt gets enhanced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184025df-5086-4e7f-8e96-9d322fe8c31a",
      "metadata": {
        "id": "184025df-5086-4e7f-8e96-9d322fe8c31a"
      },
      "source": [
        "### End of Lab 6"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}