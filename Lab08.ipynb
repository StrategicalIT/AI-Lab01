{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrategicalIT/PipedPiperAI/blob/main/Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cae130-b662-46d8-9b14-da63c83f8b31",
      "metadata": {
        "id": "a6cae130-b662-46d8-9b14-da63c83f8b31"
      },
      "source": [
        "# LAB 8: Loading and chunking data with LlamaIndex\n",
        "In this lab we are going to see how frameworks like LlamaIndex simplify common workflow tasks. In this particular example we will explore how to ingest documents. This is an example of how these frameworks aim to provide a lot of functionality with a simple interface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cf3c1c-1f52-41da-a3a1-15b2ca447f3d",
      "metadata": {
        "id": "c0cf3c1c-1f52-41da-a3a1-15b2ca447f3d"
      },
      "source": [
        "LlamaIndex provides many integrations that have been provided by the community. You can glance at what's available in LlamaHub. At the top you can use the filters to see integrations of only the type you are interested in, for example [data loaders also known as readers](https://llamahub.ai/?tab=readers). You can see how popular they are and when they were last updated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ad1ed7-f203-47a9-8f10-12a1e34e5af5",
      "metadata": {
        "id": "99ad1ed7-f203-47a9-8f10-12a1e34e5af5"
      },
      "source": [
        "In this exercise we are going to play with the [simple directory reader](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/). As you can see in its documentation page it supports many types of text files including PDF, Word and PowerPoint. It even supports some popular image, audio and video formats. All these files are treated as sources of text and automatically detected by the file extension. As you can image, all these formats are totally different from each other and the process of extracting text from them is different, but \"simpledirectoryreader\" provides a simple interface to extract data from all of them at once."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d849f9-b68b-4de3-9079-9344b68217fa",
      "metadata": {
        "id": "21d849f9-b68b-4de3-9079-9344b68217fa"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189748d-cb87-4066-83ed-8843d49e7118",
      "metadata": {
        "id": "9189748d-cb87-4066-83ed-8843d49e7118"
      },
      "source": [
        "The first step is to install the necessary libraries. In this case we will install the core llama-index package as this includes llama-index-core that simpledirectoryreader is part of. Also notice how we are installing docx2txt which is used to extract text from word documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a",
      "metadata": {
        "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index docx2txt\n",
        "!pip install llama-index-llms-nvidia llama-index-embeddings-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cac5d6e-0d59-4545-8de2-ff3969a8b3b5",
      "metadata": {
        "id": "1cac5d6e-0d59-4545-8de2-ff3969a8b3b5"
      },
      "source": [
        "First we need to import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f34438-0512-4a19-a340-f977c8964839",
      "metadata": {
        "id": "56f34438-0512-4a19-a340-f977c8964839"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2545d976-c99f-4a44-a830-b88afbdf4195",
      "metadata": {
        "id": "2545d976-c99f-4a44-a830-b88afbdf4195"
      },
      "source": [
        "For this exercise we have prepared a directory called \"data\" that contains three files: a txt, a pdf and a docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6669b9cb-0331-4ece-9939-cdf6063f2867",
      "metadata": {
        "id": "6669b9cb-0331-4ece-9939-cdf6063f2867"
      },
      "outputs": [],
      "source": [
        "# if using Google Colab, we need the /content prefix\n",
        "!ls -l /content/PipedPiperAIData/\n",
        "# if running a local notebook, then just the directory name is ok e.g. data\n",
        "#!ls -l data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695791ec-120a-4402-a550-6f23dc46c11a",
      "metadata": {
        "id": "695791ec-120a-4402-a550-6f23dc46c11a"
      },
      "source": [
        "The following line of code is sufficient to load all the data from these documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7",
      "metadata": {
        "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"/content/PipedPiperAIData/\").load_data()\n",
        "# for Google Colab be we probably need the /content prefix and point to a folder we've created at runtime with documents uploaded\n",
        "# for local notebook, probably just point directly to the folder e.g. data assuming the folder is in the same dir as this notebook run from"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325",
      "metadata": {
        "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325"
      },
      "source": [
        "Now you can simple show the contents of the \"documents\" variable to see that the text from the documents was indeed extacted. Each document in the list contains also the id and a lot of metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14432aec-a48f-42a3-83e4-470a827950d4",
      "metadata": {
        "id": "14432aec-a48f-42a3-83e4-470a827950d4"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "pprint(documents, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2978257a-c306-4543-b7b2-a522f7fd4465",
      "metadata": {
        "id": "2978257a-c306-4543-b7b2-a522f7fd4465"
      },
      "source": [
        "Feel free to experiment with other supported file types by uploading your own files. You can upload your own files using the main Jupyter interface from where you launch the notebooks.\n",
        "\n",
        "As you can see in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/), this reader offers many other possibilities like:\n",
        "- reading subdirectories with \"recursive=True\"\n",
        "- include or exclude specific paths and file extensions\n",
        "- limit the amount of files to be ingested\n",
        "It can also traverse remote file systems such as S3, Google drive, SFTP ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de6835d6-9151-4bc5-88f9-56f7adcb1a1e",
      "metadata": {
        "id": "de6835d6-9151-4bc5-88f9-56f7adcb1a1e"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9754015a-977e-4cf9-8fc0-e7f60af7b7fc",
      "metadata": {
        "id": "9754015a-977e-4cf9-8fc0-e7f60af7b7fc"
      },
      "source": [
        "## Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46460e99-6a0d-4ef1-8505-9f312b9633ef",
      "metadata": {
        "id": "46460e99-6a0d-4ef1-8505-9f312b9633ef"
      },
      "source": [
        "Chunking involves breaking down large data into smaller segments or \"chunks\". This makes the AI solution more efficient, particularly in tasks like semantic search and information retrieval. Chunking helps optimize memory usage, speeds up processing, and improves scalability.\n",
        "\n",
        "This is still an area of active research and it can be done in many ways. You can [check out the LlamaIndex documentation to see what methods are available](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/). Data scientist need to test what method is the best match for their use case"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b315891d-50e7-41ab-afa1-9e31bca33f8b",
      "metadata": {
        "id": "b315891d-50e7-41ab-afa1-9e31bca33f8b"
      },
      "source": [
        "### Fixed-sized chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4431fcd4-157b-4e9a-b2e7-479a597c5897",
      "metadata": {
        "id": "4431fcd4-157b-4e9a-b2e7-479a597c5897"
      },
      "source": [
        "This is the most basic method and it is based on a fixed amount of tokens. As you can see in the Splitter definition below we can specify how many tokens we want to target per chunk and how many tokens we want to overlap between chunks. This overlap is done to prevent information loss at chunk boundaries to ensure context preservation. This method is the often used for speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6511fd79-d41a-4f1d-8239-9a1b83d8a664",
      "metadata": {
        "id": "6511fd79-d41a-4f1d-8239-9a1b83d8a664"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "splitter = TokenTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=20,\n",
        "    separator=\" \",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2665361a-79b8-4d7e-8c6c-1f74c5fbdd31",
      "metadata": {
        "id": "2665361a-79b8-4d7e-8c6c-1f74c5fbdd31"
      },
      "source": [
        "Now we can show the chunks that were created. Notice the word count is different from 300, because the relationship between words and tokens is not 1-2-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a94ab8-e7f2-43c9-818c-3a734263833d",
      "metadata": {
        "id": "57a94ab8-e7f2-43c9-818c-3a734263833d"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "for i, _ in enumerate(nodes):\n",
        "    print(f\"=== chunk #{i}, word count:{len(nodes[i].text.split())} ===\")\n",
        "    print(nodes[i].text)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bc8192-2b8c-497b-b15c-a3d6618648af",
      "metadata": {
        "id": "54bc8192-2b8c-497b-b15c-a3d6618648af"
      },
      "source": [
        "### Recursive chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb6ba94-2137-4bef-be9d-558ec7edb3fe",
      "metadata": {
        "id": "dfb6ba94-2137-4bef-be9d-558ec7edb3fe"
      },
      "source": [
        "Here we use SentenceSplitter. This attempts to split text while respecting the boundaries of paragraphs and sentences. You can compare the results with the previous chunking method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4149e183-85b9-4a64-99b7-5b20f92fcc48",
      "metadata": {
        "id": "4149e183-85b9-4a64-99b7-5b20f92fcc48"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pprint import pprint\n",
        "\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "for i, _ in enumerate(nodes):\n",
        "    print(f\"=== chunk #{i}, word count:{len(nodes[i].text.split())} ===\")\n",
        "    print(nodes[i].text)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0677ba-50ae-4cda-9e8c-861937cf5df0",
      "metadata": {
        "id": "df0677ba-50ae-4cda-9e8c-861937cf5df0"
      },
      "source": [
        "### Semantic chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baebe8f2-1db3-4623-af9e-76768577dea0",
      "metadata": {
        "id": "baebe8f2-1db3-4623-af9e-76768577dea0"
      },
      "source": [
        "This is a relatively new concept. Instead of chunking text with a fixed chunk size, the semantic splitter adaptively picks the breakpoint in-between sentences using embedding similarity. This ensures that a \"chunk\" contains sentences that are semantically related to each other. Notice the longer time it takes to do the chunking compared to the previous two methods since the creation of embeddings and computation of cosine similiraties is more computationally expensive.\n",
        "\n",
        "The data scientist will have to experiment different values for the breakpoint percentile threshold. The higher the threshold the lower the number of breaking points, ie the less chunks.\n",
        "\n",
        "<b>IMPORTANT:</b> This algorithm makes several calls to Nvidia's embedding model. Please do not try many combinations of the threshold or you might trigger the API call throttling for your Nvidia account"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "os.environ[\"NVIDIA_API_KEY\"] = apikey\n",
        "#print(apikey)"
      ],
      "metadata": {
        "id": "67yIsWc3xP7t"
      },
      "id": "67yIsWc3xP7t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a1f1d5-2c3f-4cb8-88a5-f2dcee9ad204",
      "metadata": {
        "id": "d6a1f1d5-2c3f-4cb8-88a5-f2dcee9ad204"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
        "\n",
        "embed_model = NVIDIAEmbedding(truncate=\"END\")\n",
        "\n",
        "splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
        ")\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "for i, _ in enumerate(nodes):\n",
        "    print(f\"=== chunk #{i}, word count:{len(nodes[i].text.split())} ===\")\n",
        "    print(nodes[i].text)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184025df-5086-4e7f-8e96-9d322fe8c31a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "184025df-5086-4e7f-8e96-9d322fe8c31a"
      },
      "source": [
        "### End of Lab 8"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}